{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cosine similarity is the cosine of the angle between two vectors. In text analysis, each vector can represent a document. The greater the value of θ, the less the value of cos θ, thus the less the similarity between two documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Key Words from Financial Glossary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample regulations\n",
    "reg_1 = 'A minimum liquidation time that is five day for all other swaps'\n",
    "reg_2 = 'A maximum liquidation time that is one days for all other swaps'\n",
    "reg_3 = 'A maximum liquidation time that is six days for all other swaps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A minimum liquidation time that is five day for all other swaps\n",
      "A maximum liquidation time that is one days for all other swaps\n",
      "A maximum liquidation time that is six days for all other swaps\n"
     ]
    }
   ],
   "source": [
    "print(reg_1)\n",
    "print(reg_2)\n",
    "print(reg_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppose our list includes [minimum, maximum, liquidation, swap]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also extract all the numbers from sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reg_1 [minimum, liquidation, swap, five]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reg_2 [maximum, liquidation, swap, one]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reg_3 [maximum, liquidation, swaps, six]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reg_1 [1, 0, 1, 1, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reg_2 [0, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reg_3 [0, 1, 1, 1, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 1, 1, 5], [0, 1, 1, 1, 1], [0, 1, 1, 1, 6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "list1 = [[1, 0, 1, 1, 5], [0, 1, 1, 1, 1], [0, 1, 1, 1,6]]\n",
    "print (list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.41421356 -1.41421356  0.          0.          0.46291005]\n",
      " [-0.70710678  0.70710678  0.          0.         -1.38873015]\n",
      " [-0.70710678  0.70710678  0.          0.          0.9258201 ]]\n"
     ]
    }
   ],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(list1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.7522874 , -0.56170721],\n",
       "       [-0.7522874 ,  1.        , -0.12251278],\n",
       "       [-0.56170721, -0.12251278,  1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizing NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample regulations\n",
    "r1 = 'Reporting markets shall provide trade and supporting data reports to the Commission on a daily basis. Such reports shall include transaction-level trade data and related order information for each futures or options contract. Reports shall also include time and sales data, reference files and other information as the Commission or its designee may require. All reports must be submitted at the time, and in the manner and format, and with the specific content specified by the Commission or its designee. Upon request, such information shall be accompanied by data that identifies or facilitates the identification of each trader for each transaction or order included in a submitted trade and supporting data report if the reporting market maintains such data.'\n",
    "r2 = 'Trade and supporting data reports should be provided to the Commission daily. The reports shall include the following information: transaction-level trade data and related order information for each futures or options contract; time and sales data, reference files and other information as the Commission or its designee may require. All reports must be submitted with the specific content specified by the Commission or its designee at the time, and in the manner and format. If requested, such information shall be accompanied by data that identifies or facilitates the identification of all the trader for each submitted trade and supporting data report if available'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting markets shall provide trade and supporting data reports to the Commission on a daily basis. Such reports shall include transaction-level trade data and related order information for each futures or options contract. Reports shall also include time and sales data, reference files and other information as the Commission or its designee may require. All reports must be submitted at the time, and in the manner and format, and with the specific content specified by the Commission or its designee. Upon request, such information shall be accompanied by data that identifies or facilitates the identification of each trader for each transaction or order included in a submitted trade and supporting data report if the reporting market maintains such data.\n",
      "\n",
      "Trade and supporting data reports should be provided to the Commission daily. The reports shall include the following information: transaction-level trade data and related order information for each futures or options contract; time and sales data, reference files and other information as the Commission or its designee may require. All reports must be submitted with the specific content specified by the Commission or its designee at the time, and in the manner and format. If requested, such information shall be accompanied by data that identifies or facilitates the identification of all the trader for each submitted trade and supporting data report if available\n"
     ]
    }
   ],
   "source": [
    "print(r1)\n",
    "print()\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r1 can be viewed as external regulation and r2 can be viewed as internal policy. We would like to calculate the distance between two sentences to see if those regulations match with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Raw texts are preprocessed with the most common words and punctuation removed, tokenization, and stemming (or lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. A dictionary of unique terms found in the whole corpus is created. Texts are quantified first by calculating the term frequency (tf) for each document. The numbers are used to create a vector for each document where each component in the vector stands for the term frequency in that document. Let n be the number of documents and m be the number of unique terms. Then we have an n by m tf matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. The core of the rest is to obtain a “term frequency-inverse document frequency” (tf-idf) matrix. Inverse document frequency is an adjustment to term frequency. This adjustment deals with the problem that generally speaking certain terms do occur more than others. Thus, tf-idf scales up the importance of rarer terms and scales down the importance of more frequent terms relative to the whole corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. The calculated tf-idf is normalized by the Euclidean norm so that each row vector has a length of 1. The normalized tf-idf matrix should be in the shape of n by m. A cosine similarity matrix (n by n) can be obtained by multiplying the if-idf matrix by its transpose (m by n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing with nltk\n",
    "# Normalize by stemming\n",
    "documents = [r1, r2]\n",
    "import nltk, string, numpy\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "def StemTokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def StemNormalize(text):\n",
    "    return StemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize by lemmatization\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x44 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 76 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Turn text into vectors of term frequency:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "LemVectorizer = CountVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "LemVectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reporting': 29, 'market': 21, 'shall': 34, 'provide': 24, 'trade': 40, 'supporting': 38, 'data': 7, 'report': 28, 'commission': 3, 'daily': 6, 'basis': 2, 'include': 16, 'transactionlevel': 43, 'related': 27, 'order': 23, 'information': 18, 'future': 13, 'option': 22, 'contract': 5, 'time': 39, 'sale': 33, 'reference': 26, 'file': 10, 'designee': 8, 'require': 32, 'submitted': 37, 'manner': 20, 'format': 12, 'specific': 35, 'content': 4, 'specified': 36, 'request': 30, 'accompanied': 0, 'identifies': 15, 'facilitates': 9, 'identification': 14, 'trader': 41, 'transaction': 42, 'included': 17, 'maintains': 19, 'provided': 25, 'following': 11, 'requested': 31, 'available': 1}\n"
     ]
    }
   ],
   "source": [
    "# Normalized (after lemmatization) text in the four documents are tokenized and each term is indexed:\n",
    "print (LemVectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 3 1 1 1 6 2 1 1 0 1 1 1 1 2 1 3 1 1 2 1 2 1 0 1 1 5 2 1 0 1 1 4 1\n",
      "  1 2 2 2 3 1 1 1]\n",
      " [1 1 0 3 1 1 1 5 2 1 1 1 1 1 1 1 1 0 4 0 1 0 1 1 0 1 1 1 4 0 0 1 1 1 2 1\n",
      "  1 2 2 2 3 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Tf-matrix\n",
    "tf_matrix = LemVectorizer.transform(documents).toarray()\n",
    "print (tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 44)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be 2 by 44 \n",
    "tf_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.40546511 1.40546511 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.40546511\n",
      " 1.         1.         1.         1.         1.         1.40546511\n",
      " 1.         1.40546511 1.         1.40546511 1.         1.\n",
      " 1.40546511 1.40546511 1.         1.         1.         1.40546511\n",
      " 1.40546511 1.40546511 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.40546511 1.        ]\n",
      "\n",
      "[[0.0754519  0.         0.10604501 0.2263557  0.0754519  0.0754519\n",
      "  0.0754519  0.45271139 0.1509038  0.0754519  0.0754519  0.\n",
      "  0.0754519  0.0754519  0.0754519  0.0754519  0.1509038  0.10604501\n",
      "  0.2263557  0.10604501 0.0754519  0.21209002 0.0754519  0.1509038\n",
      "  0.10604501 0.         0.0754519  0.0754519  0.37725949 0.21209002\n",
      "  0.10604501 0.         0.0754519  0.0754519  0.3018076  0.0754519\n",
      "  0.0754519  0.1509038  0.1509038  0.1509038  0.2263557  0.0754519\n",
      "  0.10604501 0.0754519 ]\n",
      " [0.08947804 0.12575827 0.         0.26843413 0.08947804 0.08947804\n",
      "  0.08947804 0.44739021 0.17895608 0.08947804 0.08947804 0.12575827\n",
      "  0.08947804 0.08947804 0.08947804 0.08947804 0.08947804 0.\n",
      "  0.35791217 0.         0.08947804 0.         0.08947804 0.08947804\n",
      "  0.         0.12575827 0.08947804 0.08947804 0.35791217 0.\n",
      "  0.         0.12575827 0.08947804 0.08947804 0.17895608 0.08947804\n",
      "  0.08947804 0.17895608 0.17895608 0.17895608 0.26843413 0.08947804\n",
      "  0.         0.08947804]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate idf and turn tf matrix to tf-idf matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfTran = TfidfTransformer(norm=\"l2\")\n",
    "tfidfTran.fit(tf_matrix)\n",
    "print (tfidfTran.idf_)\n",
    "print()\n",
    "tfidf_matrix = tfidfTran.transform(tf_matrix)\n",
    "print (tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.86416488]\n",
      " [0.86416488 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity\n",
    "cos_similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
    "print (cos_similarity_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
